---
title: 阿里DIN(Deep Interest Network)阅读笔记
date: 2021/3/12 13:52
tags: 深度学习
categories: Deep Learning
excerpt: Deep Interest Network(DIN)是阿里的精准定向检索及基础算法团队，在2017年6月提出的。它针对电子商务领域(e-commerce industry)的CTR预估，充分利用/挖掘用户历史行为数据中的信息对当前数据进行预估，对笔者的轨迹预测研究有一定的借鉴意义。
password: 1021
---

# Deep Interest Network (DIN)
## DIN背景
### 研究数据(对象)
在电子商务领域，用户的购物行为具有以下数据分布特点：

1. Diversity (多样性，差异性)：用户在访问电商网站时会对多种商品都感兴趣。可以总结为用户兴趣的广泛特征。
2. Local Activation (局部活跃)：用户是否会点击推荐给他的商品，仅仅取决于历史行为数据中的一小部分，而不是全部。即历史对当前预估有帮助，但并非全部必要，可归纳为用户历史行为的针对性特征。

**举例：**
Diversity：用户购物多样性往往体现在购物种类上，用户购物的商品往往涉及多个类别，例如衣物、生活用品、娱乐设施等。
Local Activation：尽管用户购物具有多样性，但是针对用户当前的购物行为，部分历史记录是无效的，例如用户在最近购买了一整套潜水设备，那么他接下来一步的购买计划按理说应该也与设备，游泳等关键字有关，与他过往购买食品，电器等记录没有任何关系。

### 解决方案
用户购物行为差异巨大，如何正确识别并推断用户的购物行为规律是模型需要解决的问题，DIN给出了以下解决方案：

1. **学习用户的兴趣分布来表示用户多种多样的兴趣爱好**
2. **使用Attention机制来实现Local Activation**
3. 针对模型训练，提出了Dice激活函数，自适应正则，显著提升了模型性能与收敛速度

## DNN实现CTR预估
随着深度学习在CV、NLP等领域取得突破性进展，一些研究也开始尝试将DNN应用于CTR预估，比如：Wide&Deep, DeepFM等。
**普遍思路：**

1. 输入Embedding: 在输入上面加一层embeding层，把最原始高维度、稀疏的数据转换为低维度的实值表示上(dense vector)。
2. 全连接层：增加多个全连接层，学习特征之间的非线性关系。

DNN实现CTR预估的整体流程：
`Sparse Features -> Embedding Vector -> MLPs -> Output`

**优势：**
与人工挖掘数据特征相比，大大**减少了人工特征工程的工作量**。不需要做特殊的特征组合/特征交叉，整个过程通过DNN去学习特征间的交互信息。

**缺点：**
在对用户的历史行为数据(User Behavior Data)分析时，没有针对性利用其中包含的大量用户兴趣信息。

## DIN改进
**针对Diversity**：
针对用户广泛的兴趣，DIN用an interest distribution去表示。

**针对Local Activation：**
DIN借鉴机器翻译中的Attention机制，设计了一种attention-like network structure， 针对当前候选Ad，去局部的激活(Local Activate)相关的历史兴趣信息。和当前候选Ad相关性越高的历史行为，会获得更高的attention score，从而会主导这一次预测。

### DIN训练数据：
**特点：**

1. 多样性：数据类别多，关系复杂
2. 局部活跃：针对当前预测，只有部分历史数据值得被关注
3. 高维度&稀疏性：多值离散特征。当前数据与整体数据相比，过于稀疏，举例：用户在YouTube上看的视频和搜索过的视频。无论是看过的还是搜索过的，都不止一个，但是相对于所有的视频来说，看过和搜索过的数量都太小了(非常稀疏)。
4. 历史行为长度不可控：以电子商务为例，用户购买过的good_id有多个，购买过的shop_id也有多个，而直接导致了每个用户的历史行为id长度是不同的

**解决方法：**

1. 多样性：an interest distribution
2. 局部活跃：设计了一种attention-like network structure。Attention机制来源于Neural Machine Translation(NMT)。DIN使用Attention机制去更好的建模局部激活。在DIN场景中，针对不同的候选广告需要自适应地调整User Representation。也就是说：在Embedding Layer -> Pooling Layer得到用户兴趣表示的时候，**赋予不同的历史行为不同的权重，实现局部激活**。从最终反向训练的角度来看，就是根据当前的候选广告，来反向的激活用户历史的兴趣爱好，赋予不同历史行为不同的权重。
3. 高维度&稀疏：Embedding。通过 embedding layer 实现高维度、稀疏数据向低维度、稠密向量转化。
4. 历史行为长度：在Embedding Layer后面增加一个Pooling Layer。Pooling可以用sum或average。最终得到一个固定长度的Embedding Vector，是用户兴趣的一个抽象表示，常被称作User Representation。缺点是会损失一些信息，DIN用Attention弥补了这一缺点。

### DIN原理
#### 应用场景：
用户访问阿里巴巴的电商网站，看到推荐的广告时，大部分人都是**没有一个明确的目标的**，他自己也不知道要买什么。所以，用一个高效的方法从用户丰富的历史行为数据中获取用户的兴趣信息并进行推荐，就非常关键了。(总的来说，当用户没有明确目标时，推荐系统的价值是最大的)

#### Base Model
Base Model主要由两部分组成：

1. 把稀疏的输入(id特征)转换成embedding vector
2. 增加MLPs得到最终的输出

![](/img/posts_img/20210312151315988_31436.png)
如图所示：不同用户的历史购物记录是不一样的，因此输入模型的序列长度也不同。因此，DIN在Embedding Layer和MLPs中间，增加了一个Pooling Layer，使用的是sum operation，把这些goods或shops的embedding vector相加，得到一个固定长度的向量作为MLPs的输入。

#### DIN Model
在 Base Model 进行 Pooling 操作时，实际上损失了很多信息，DIN 设计了一个 Activation Unit 来实现局部激活，对 Local Activation 建模。

##### 模型目标
**基于用户历史行为，充分挖掘用户兴趣和候选广告之间的关系**。用户是否点击某个广告往往是基于他之前的部分兴趣，这是应用Attention机制的基础。**Attention**机制简单的理解就是**对于不同的特征有不同的权重**，这样某些特征就会**主导**这一次的**预测**，就好像模型对某些特征pay attention。但是，DIN中并不能直接用attention机制。因为对于不同的候选广告，用户兴趣表示(embedding vector)应该是不同的。也就是说**用户针对不同的广告表现出不同的兴趣表示**，即使历史兴趣行为相同，但是各个行为的权重不同。

##### Local Activation 理解
1. 所有数据都会被映射到Embedding空间中。他们两者的关系，是在Embedding空间中学习的。
2. 现在有用户U和两个候选广告 A,B。在Embedding空间中，U和A，U和B的相关性都比较高。假设我们使用内积来计算用户和广告之间的相关性。广告A和B嵌入后的向量分别为Va, Vb，那么就要求对于Va Vb终点连线上的任何一个广告，用户U都要有很高的相关性。(通俗理解就是，通过(U,A,B)间的内积关系所推荐的商品要求是用户的正选择，要发现这样的关系是非常困难的)，这样的限制使得模型非常难学习到有效的用户和广告的embedidng表示。当然，如果增大embedding的大小，也许能行。但是会极大的增加模型参数数量。

阿里团队认为：用户Embedding Vector的维度为k(即用户历史购买商品个数)，它最多表示k个相互独立的兴趣爱好。但是用户的兴趣爱好远远不止k个。即用Embedding向量间相关性是无法反映用户整体兴趣爱好的(即用户购买记录并非用户想要购买的全部)。
因此，DIN给出了一套方案：**用户兴趣不再是一个点，而是一个一个分布，一个多峰的函数，一个峰就表示一个兴趣，峰值的大小表示兴趣强度。**那么针对不同的候选广告，用户的兴趣强度是不同的，也就是说随着候选广告的变化，用户的兴趣强度不断在变化。。这样即使在低维空间，也可以获得几乎无限强的表达能力。
假定用户兴趣表示的Embedding Vector是Vu，候选广告的是Va，那么Vu是Va的函数。 也就是说，用户针对不同的广告有不同的用户兴趣表示(嵌入向量不同)。
![](/img/posts_img/20210312153715049_16477.png)
其中，`Vi`表示behavior id i的嵌入向量，比如good_id,shop_id等。`Vu`是所有behavior ids的加权和，表示的是用户兴趣。候选广告影响着每个behavior id的权重，也就是Local Activation。权重表示的是：每一个behavior id针对当前的候选广告`Va`，对总的用户兴趣表示的Embedding Vector的贡献大小。在实际实现中，权重用激活函数的输出来表示，输入是`Vi`和`Va`。


## DIN在轨迹预测上的借鉴意义
### 数据角度
**DIN**
用户历史购物记录；系统当前商品推荐
(商品多样，稀疏，历史序列长度不固定，针对当前推荐只有部分历史记录有价值)
**轨迹预测(基于当前轨迹和历史轨迹)**
驾驶者历史出行记录；驾驶者当前正在出行轨迹
(将轨迹点看成一个商品，则轨迹也存在多样，稀疏等特征，驾驶者不同天历史出行轨迹数目，长度等都不固定，针对当前出行只有部分历史轨迹可能有相关性)

### 应用角度
**DIN**
用户没有明确购物目标时，推荐系统可以根据用户以往的历史购物记录为用户推荐最有购买欲望的商品
**轨迹预测(基于当前轨迹和历史轨迹)**
驾驶者的出行往往是有计划的，这点与商品购物不同，但是针对目的地的具体范围和到达目的地后的下一步行为不一定有详细的计划，此时轨迹预测就可以在驾驶者行驶过程中实时预测本次出行目的地，并及时获取目的地周围的一系列具有商业价值的信息，辅助驾驶者在行进过程中对本次驾驶计划做进一步完善。

### 结构/原理
**DIN**

1. embedding 层：降维多维离散特征，映射到 embedding 空间，获得稠密向量。
2. AU || Local Activation || 多峰函数：通过当前候选广告，局部激活用户有效的历史行为，随着当前候选广告变化，用户兴趣强度也会不断进行调整。
3. sum pooling：用户历史序列长度不一致，用sum统一到固定长度向量。

**轨迹预测(基于当前轨迹和历史轨迹)**

1. embedding 层：降维多维离散特征，映射到 embedding 空间，获得稠密向量。
2. AU || 多峰函数：在轨迹预测中没有用到 Local Activation 概念，在商品推荐中，要针对不同的用户，各用户间不能混淆，因此引入了 Local Activation。本研究轨迹预测针对单用户，因此不需要 Local Activaiton 加以区分。
3. gru-hidden：用户历史出行的距离，一天内出行次数，轨迹点数目等都是不固定且动态变化的。针对时序数据，研究采用gru提取历史各天的规律信息来统一向量长度，此外针对当天出行的动态性，采用gru提取了当前出行的各时间步隐式规律。至此，所有向量输出大小均被限制在 hidden-size。

### 轨迹预测的多峰函数
![](/img/posts_img/20210312153715049_16477.png)
和DIN对应，在轨迹预测中，`Vi`表示驾驶者各天历史出行的总体规律(每天gru的最后一个时间步)，此时`N=7`表示历史出行记录时间跨度为一周，`Va`表示当前出行的规律，权重`wi=g(Vi,Va)`表示某个历史规律针对当前出行规律，计算得到对总体驾驶者出行表示的Embedding Vector的贡献大小。`Vu`是驾驶者历史一周内所有规律的加权和，代表了驾驶者周期长度为一周的出行规律的总结。
值得注意的是，本研究针对当前出行取了各时间步(节点)的规律，这很好的模拟了驾驶者出行的动态性，随着驾驶者行为的不断变化，其一周内历史规律内部的"兴趣"强度也在不断地变化、更新，从而能在最后一个当前时间步(当前点)更加准确地预测目的地。